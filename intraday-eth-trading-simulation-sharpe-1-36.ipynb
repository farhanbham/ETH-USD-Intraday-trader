{"metadata":{"colab":{"collapsed_sections":["nxXpPdVdWHv0","iScbE9mVPg6W","xv7Y7zMuWNzt","zD_9w8mk59O-","ANo9dXWwpNYb"],"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8819887,"sourceType":"datasetVersion","datasetId":5306016},{"sourceId":89298,"sourceType":"modelInstanceVersion","modelInstanceId":74910,"modelId":99632}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preprocessing\n1. Import Libraries and download data\n2. Scale training and test data using training data parameters to ensure no data leakage\n3. Creates Sequences for training\n4. Utilise Encoder-Decoder Network to reduce dimensionality of data to reduce overfitting","metadata":{"_uuid":"56c1ac9b-1a41-477d-bd2d-e70f695f9dc6","_cell_guid":"d11f685f-11a1-463a-b682-2aa058d74896","id":"nxXpPdVdWHv0","trusted":true}},{"cell_type":"code","source":"random_state = 1\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Layer, Dropout, BatchNormalization, Bidirectional, TimeDistributed\nfrom tensorflow.keras.optimizers import AdamW, SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.losses import MAE","metadata":{"execution":{"iopub.status.busy":"2024-08-04T14:41:10.681509Z","iopub.execute_input":"2024-08-04T14:41:10.682361Z","iopub.status.idle":"2024-08-04T14:41:10.690207Z","shell.execute_reply.started":"2024-08-04T14:41:10.682330Z","shell.execute_reply":"2024-08-04T14:41:10.689331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read dataset and set index to datetime for plotting purposes\ndata = pd.read_csv(\"/kaggle/input/eth-all/ETH_return2.csv\")\ndata[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\ndata.set_index(\"timestamp\", inplace=True)\n\n\n# Scale data using train parameters to ensure no data leakage\nsplit = int(len(data) * 0.8)\ntrain, test = data[:split], data[split:]\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train)\ntest_scaled = scaler.transform(test)\nscaled_data = np.vstack((train_scaled, test_scaled))\n\n\n\n# Create Sequences\ncolumn_index = data.columns.get_loc('close')\npredict_value = 24\nSEQ_LENGTH = 24\ndef create_sequences(data, seq_length):\n    X, Y = [], []\n    for i in range(len(data) - seq_length - predict_value):  \n        x = data[i:i + seq_length]\n        y = data[i + seq_length + predict_value, column_index]  \n        X.append(x)\n        Y.append(y)\n    return np.array(X), np.array(Y)\nX, y = create_sequences(scaled_data, SEQ_LENGTH)\n\n\nsplit = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]","metadata":{"_uuid":"7bfc2347-b063-4538-88e7-59ddaba6962d","_cell_guid":"702697a8-a1c4-436d-91a2-b39f9a227e3c","execution":{"iopub.status.busy":"2024-08-04T16:44:19.341355Z","iopub.execute_input":"2024-08-04T16:44:19.342080Z","iopub.status.idle":"2024-08-04T16:44:19.627644Z","shell.execute_reply.started":"2024-08-04T16:44:19.342044Z","shell.execute_reply":"2024-08-04T16:44:19.626604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto-Encoder","metadata":{}},{"cell_type":"code","source":"# Encoder - Decoder\ndropout_rate = 0.2\nactivation = \"tanh\"\nLSTM_units = 350\nlearning_rate = 0.1\n\ninput_layer = Input(shape=(X.shape[1], X.shape[2]))\nencoded = LSTM(LSTM_units, activation=activation, return_sequences=True)(input_layer)\nencoded = LSTM(4, activation=activation, return_sequences=True)(encoded)\ndecoded = LSTM(LSTM_units, return_sequences=True, activation=activation)(encoded)\ndecoded = Dropout(dropout_rate)(decoded)\ndecoded = TimeDistributed(Dense(X.shape[2]))(decoded)\n\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer=SGD(learning_rate=learning_rate, momentum=0.5), loss=MAE)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nhistory = autoencoder.fit(X, X, epochs=100, batch_size=128, validation_split=0.2, callbacks = [early_stopping])\n\nencoder = Model(input_layer, encoded)\nencoded_data = encoder.predict(X)\n\n\n# Testing performance of encoder\nreconstructed_sequences = autoencoder.predict(X)\nmse = np.mean(np.power(X - reconstructed_sequences, 2), axis=(1, 2))\nprint(f'Mean Squared Error: {np.mean(mse)}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:54:25.925887Z","iopub.execute_input":"2024-08-04T16:54:25.926576Z","iopub.status.idle":"2024-08-04T17:00:44.654504Z","shell.execute_reply.started":"2024-08-04T16:54:25.926543Z","shell.execute_reply":"2024-08-04T17:00:44.653511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Forecasting Model","metadata":{"_uuid":"2c0f22bf-8a86-4b93-996b-2f07cc9cb33e","_cell_guid":"68e5ad46-82a8-4bba-9d8e-f31963d5925f","id":"xv7Y7zMuWNzt","trusted":true}},{"cell_type":"code","source":"split = int(len(encoded_data) * 0.8)\nencoded_X_train, encoded_X_test = encoded_data[:split], encoded_data[split:]\n\nlstm_units = 12\ndropout_rate = 0.1\ndense_units = 128\nlearning_rate = 0.001\nweight_decay = 0.01\nregularizer = 0.00001\nnum_layers = 2\nactivation = 'tanh'\n\ninput_seq = Input(shape=(SEQ_LENGTH, encoded_X_train.shape[2]))\nfor _ in range(num_layers):\n    x = Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2(weight_decay), recurrent_regularizer=l2(regularizer)))(input_seq)\n    x = Dropout(dropout_rate)(x)\n\nx = Bidirectional(LSTM(lstm_units, return_sequences=False, kernel_regularizer=l2(weight_decay), recurrent_regularizer=l2(regularizer)))(x)\nx = Dropout(dropout_rate)(x)\nx = BatchNormalization()(x) \nx = Dropout(dropout_rate)(x)\noutput = Dense(1)(x)\n\nmodel = Model(inputs=input_seq, outputs=output)\nmodel.compile(optimizer=SGD(learning_rate=learning_rate, momentum=0.9), loss=MAE)\n\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\nhistory = model.fit(encoded_X_train, y_train, epochs=100, validation_split = 0.2, batch_size=128, callbacks = [early_stopping])\n#model.save('Predict_24hr_model2.h5')","metadata":{"_uuid":"4fb2cc9d-f8d7-4d51-a68d-cdcab6325525","_cell_guid":"709881cf-0613-46e5-9a99-b023eff610bf","id":"wdM3jU1faj8v","execution":{"iopub.status.busy":"2024-08-04T17:01:59.178402Z","iopub.execute_input":"2024-08-04T17:01:59.179208Z","iopub.status.idle":"2024-08-04T17:05:11.526210Z","shell.execute_reply.started":"2024-08-04T17:01:59.179174Z","shell.execute_reply":"2024-08-04T17:05:11.525281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation of Predictions","metadata":{"_uuid":"0526d7ff-34d6-4d97-8951-03e5bdc07a0e","_cell_guid":"8052459c-c662-4b46-a496-f425041b08c0","trusted":true}},{"cell_type":"code","source":"#model = load_model(\"/kaggle/working/Predict_24hr.h5\")\n\ntest_predictions = model.predict(encoded_X_test)\n\n# This code is here to reset the datasets when running code multple times in one session\nX, y = create_sequences(scaled_data, SEQ_LENGTH)\nsplit = int(0.8 * len(X))\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n\n# Inverse Scale Values\ny_test_partial_reshaped = y_test.reshape(-1, 1)\ny_test_reshaped = np.repeat(y_test_partial_reshaped, X_train.shape[2], axis=1)\npreds_reshaped = np.repeat(test_predictions, X_train.shape[2], axis=1)\ny_pred = scaler.inverse_transform(preds_reshaped)[:,column_index]\ny_test = scaler.inverse_transform(y_test_reshaped)[:,column_index]\n\n\n# Plot Preds vs Targets\nplt.plot(y_test, label='Test Actual', color='orange')\nplt.plot(y_pred, label='Test Predicted', color='red')\nplt.xlabel('Time (Hours)')\nplt.ylabel('Price')\nplt.title('Actual vs Predicted Values')\nplt.legend()\nplt.show()\n\n\n# Metrics\ndef MAPE(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\ntest_mse = mean_squared_error(y_test, y_pred)\ntest_mae = mean_absolute_error(y_test, y_pred)\ntest_mape = MAPE(y_test, y_pred)\nprint(f\"Test MSE: {test_mse}\")\nprint(f\"Test MAE: {test_mae}\")\nprint(f\"Test MAPE: {test_mape}\")","metadata":{"_uuid":"83d9413f-be5a-40aa-90a8-c9920c87e1b7","_cell_guid":"1ab81bf6-c027-4fa7-ab13-b2d1eb69ccd4","execution":{"iopub.status.busy":"2024-08-04T17:05:44.507798Z","iopub.execute_input":"2024-08-04T17:05:44.508757Z","iopub.status.idle":"2024-08-04T17:05:46.971765Z","shell.execute_reply.started":"2024-08-04T17:05:44.508722Z","shell.execute_reply":"2024-08-04T17:05:46.970822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comparing predicted price change to actual price change in 24 hours**","metadata":{}},{"cell_type":"code","source":"hours = 72\n\npredicted_change = (y_pred[hours:] / y_test[:-hours]) - 1\nactual_change = (y_test[hours:] / y_test[:-hours]) - 1\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 4)) \nax[0].hist(predicted_change, bins=50)\nax[0].set_title(f\"Predicted Price Change in {hours} hours (%)\")\nax[0].set_xlabel(\"% change\")\n\nax[1].hist(actual_change, bins=50)\nax[1].set_title(f\"Actual Price Change in {hours} hours (%)\")\nax[1].set_xlabel(\"% change\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"09220e81-7016-492e-b59a-e23e696a4bdd","_cell_guid":"fe76383b-31d1-4bda-97dc-61e4ff887825","execution":{"iopub.status.busy":"2024-08-04T15:16:28.395380Z","iopub.execute_input":"2024-08-04T15:16:28.395904Z","iopub.status.idle":"2024-08-04T15:16:29.065772Z","shell.execute_reply.started":"2024-08-04T15:16:28.395870Z","shell.execute_reply":"2024-08-04T15:16:29.064841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Identifying Performance in prediciting price direction**","metadata":{}},{"cell_type":"code","source":"# Transforming Preds and Targets to % difference\npredicted_direction = []\nactual_direction = []\n\n# Transforming preds and targets to 1 if upwards move and 0 if downwards move\nfor i in range(0,len(y_test)-predict_value,predict_value):\n    if y_test[i+predict_value] >= y_test[i]:\n        predicted_direction.append(1)\n    else:\n        predicted_direction.append(0)\n        \nfor i in range(0,len(y_pred)-predict_value,predict_value):\n    if y_pred[i+predict_value] >= y_test[i]:\n        actual_direction.append(1)\n    else:\n        actual_direction.append(0)\n\n# Plotting Confusion Matrix\nmatrix = metrics.confusion_matrix(actual_direction, predicted_direction)\ncm_display = metrics.ConfusionMatrixDisplay(matrix, display_labels = [\"down\", \"up\"])\ncm_display.plot()\nplt.title(\"Prediction of Upward and Downward Price Movements\")\nplt.show()","metadata":{"_uuid":"798a0429-0fd6-4792-bec6-68bf57828e26","_cell_guid":"89abcd73-2888-4d50-803f-d5c7ec7291a7","execution":{"iopub.status.busy":"2024-08-04T17:05:56.954377Z","iopub.execute_input":"2024-08-04T17:05:56.955228Z","iopub.status.idle":"2024-08-04T17:05:57.225339Z","shell.execute_reply.started":"2024-08-04T17:05:56.955194Z","shell.execute_reply":"2024-08-04T17:05:57.224503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VaR of Returns (Historical Estimation)**","metadata":{}},{"cell_type":"code","source":"confidence_level = 0.975\nfor x in range(24,97,24):\n    pct_change = y_test[x:] / y_test[:-x]\n    var_hist = 1 - np.percentile(pct_change, (1 - confidence_level) * 100)   \n    print(f\"{confidence_level * 100}% confident price will not fall more than {var_hist:.2%} in {x} hours\")\nprint(\" \")\nconfidence_level = 0.95\nfor x in range(24,97,24):\n    pct_change = y_test[x:] / y_test[:-x]\n    var_hist = 1 - np.percentile(pct_change, (1 - confidence_level) * 100)   \n    print(f\"{confidence_level * 100}% confident price will not fall more than {var_hist:.2%} in {x} hours\")","metadata":{"_uuid":"8db677b0-83db-4618-847a-c14725240d1b","_cell_guid":"f710cb2c-6027-4d7c-bc26-bed3a0815bda","execution":{"iopub.status.busy":"2024-08-04T15:16:36.009563Z","iopub.execute_input":"2024-08-04T15:16:36.010566Z","iopub.status.idle":"2024-08-04T15:16:36.021361Z","shell.execute_reply.started":"2024-08-04T15:16:36.010523Z","shell.execute_reply":"2024-08-04T15:16:36.020397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trading Simulation","metadata":{"_uuid":"a53f0c14-fa06-4ff1-a5fc-e0868ee5a60e","_cell_guid":"d93c23a3-7bf8-47d4-8800-5a5054ba2347","trusted":true}},{"cell_type":"code","source":"prices = y_test[:-24]\npreds = y_pred[24:]\nactual = y_test[24:]\n\ninitial_balance = 10000\nslippage = 0.0015  # binance trading fee (0.1%) + margin loss (0.05%)\nbalance = initial_balance\nholdings = 0\nbuy_price = 0\nprofits = []\nportfolio_value = []\nactions = []\n\n\n\nposition_size = 1\ntake_profit = 1.1\nmax_hold_time = 48\nsignal_strength = 1.06\n\nfor i in range(len(prices)):\n\n    current_price = prices[i]\n    predicted_price = preds[i]\n    signal = predicted_price / current_price\n    action = \"HOLD\"\n    \n    if signal > signal_strength and balance > 0:\n        amount_to_invest = balance*position_size\n        shares_to_buy = amount_to_invest / current_price\n        transaction_cost = shares_to_buy * current_price * slippage\n        actual_shares_bought = (amount_to_invest - transaction_cost) / current_price\n        holdings += actual_shares_bought\n        balance -= amount_to_invest\n        buy_price = current_price\n        action = 'BUY'   \n\n    elif current_price > buy_price*take_profit and holdings > 0:\n            transaction_cost = holdings * current_price * slippage\n            balance += (holdings * current_price) - transaction_cost\n            profits.append(holdings * (current_price - buy_price) - transaction_cost)\n            holdings = 0\n            action = \"PROFIT SELL\"\n        \n        \n    if i % max_hold_time == 0 :\n        if holdings > 0:\n            transaction_cost = holdings * current_price * slippage\n            balance += (holdings * current_price) - transaction_cost\n            profits.append(holdings * (current_price - buy_price) - transaction_cost)\n            holdings = 0\n            action = \"TIME SELL\"\n    \n\n    actions.append(action)  \n    portfolio_value.append(balance + (holdings * current_price))\n\nportfolio_value = np.array(portfolio_value)\nportfolio = (portfolio_value * 100 / portfolio_value[0]) - 100\nstrategy_profit = ((portfolio_value[-1] / initial_balance) * 100) - 100\n\nbuy_and_hold = (prices * (100 - slippage) / prices[0]) - 100\nbuy_and_hold_profit = ((prices[-1] / prices[0]) * 100-slippage) - 100 \n\n\nprint(f\"Strategy Profit: {strategy_profit:.2f}%\")\nprint(f\"Buy and Hold Profit: {buy_and_hold_profit:.2f}%\")\n\nindex = test.index[predict_value+10:]\nplt.figure(figsize=(9, 3))\nplt.plot(index, portfolio, label=\"Strategy\")\nplt.plot(index, buy_and_hold, label=\"Buy And Hold\") \nplt.xlabel('Time')\nplt.ylabel('Profit (%)')\nplt.title('Strategy vs Buy and Hold')\nplt.legend()\nplt.show()\n\nprint(pd.Series(actions).value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-08-04T17:06:02.812449Z","iopub.execute_input":"2024-08-04T17:06:02.813314Z","iopub.status.idle":"2024-08-04T17:06:03.230298Z","shell.execute_reply.started":"2024-08-04T17:06:02.813282Z","shell.execute_reply":"2024-08-04T17:06:03.229500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Strategy Evaluation","metadata":{"_uuid":"4ca85fed-d7e2-4d94-a972-053502571e0c","_cell_guid":"4850a2b2-ed2f-4229-8f51-9b0dc159baf9","trusted":true}},{"cell_type":"code","source":"!pip install -q pandas pandas_datareader\nimport pandas_datareader.data as web\nfrom datetime import datetime\n\ndf = web.DataReader('GS1', 'fred', datetime(2023, 1, 23),datetime(2024,6,2))\nrf_annual = np.mean(df)/100\nprint(f\"Risk Free Rate: {rf_annual}\")","metadata":{"_uuid":"925dcf3d-d1a8-4017-8dba-fd13f13c7c27","_cell_guid":"0648ea13-7a07-435c-bd18-f46b19fe8652","execution":{"iopub.status.busy":"2024-08-04T14:45:51.116760Z","iopub.execute_input":"2024-08-04T14:45:51.117116Z","iopub.status.idle":"2024-08-04T14:46:07.610900Z","shell.execute_reply.started":"2024-08-04T14:45:51.117088Z","shell.execute_reply":"2024-08-04T14:46:07.609875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PL_returns = np.diff(np.log(np.array(portfolio_value)))\nmarket_returns = np.diff(np.log(np.array(y_test)))\n\ndef calc_max_drawdown(values):\n    balances_series = pd.Series(values)\n    running_max = balances_series.cummax()\n    drawdown = (balances_series - running_max) / running_max\n    max_drawdown = drawdown.min()\n    return -max_drawdown*100\n\ndef calculate_metrics(values, rf):\n    annual_return = np.mean(values)*365*24\n    volatility = np.std(values)*np.sqrt(365*24)\n    max_drawdown = np.max(np.maximum.accumulate(values) - values) / np.max(np.maximum.accumulate(values))\n    sharpe_ratio = (annual_return - rf) / volatility\n    return volatility, max_drawdown, sharpe_ratio, annual_return\n\ntrading_metrics = calculate_metrics(PL_returns,rf_annual)\nbuy_and_hold_metrics = calculate_metrics(market_returns,rf_annual)\ntrading_drawdown = calc_max_drawdown(portfolio_value)\nbuy_and_hold_drawdown = calc_max_drawdown(prices)\n\n# Print metrics\nprint(\"Trading Strategy Metrics:\")\nprint(f\"Annual Return: {trading_metrics[3]*100:.2f}%\")\nprint(f\"Volatility: {trading_metrics[0]:.2f}\")\nprint(f\"Maximum Drawdown: {trading_drawdown:.2f}%\")\nprint(f\"Sharpe Ratio: {trading_metrics[2]:.2f}\")\n\nprint(\"\\nBuy and Hold Metrics:\")\nprint(f\"Annual Return: {buy_and_hold_metrics[3]*100:.2f}%\")\nprint(f\"Volatility: {buy_and_hold_metrics[0]:.2f}\")\nprint(f\"Maximum Drawdown: {buy_and_hold_drawdown:.2f}%\")\nprint(f\"Sharpe Ratio: {buy_and_hold_metrics[2]:.2f}\")","metadata":{"_uuid":"b9a82ff6-0d76-4ba6-9180-679a223673c4","_cell_guid":"c8a673af-dc78-4897-b77f-df21186ed49a","execution":{"iopub.status.busy":"2024-08-04T17:09:18.805774Z","iopub.execute_input":"2024-08-04T17:09:18.806163Z","iopub.status.idle":"2024-08-04T17:09:18.821682Z","shell.execute_reply.started":"2024-08-04T17:09:18.806131Z","shell.execute_reply":"2024-08-04T17:09:18.820595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling Prices in various market dynamics","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n# Assuming 'data' is your DataFrame with a DatetimeIndex\n\nlog_returns = pd.Series(np.diff(np.log(data[\"close\"])))\nlog_returns = log_returns[np.isfinite(log_returns)]\nmarket_conditions = {\n    'flat': log_returns.loc['2018-01-01 00:00:00':'2021-01-01 00:00:00'],\n    'bull': log_returns.loc['2021-01-01 00:00:00':'2022-01-01 00:00:00'],\n    'bear': log_returns.loc['2022-01-01 00:00:00':'2023-01-01 00:00:00'],\n    'test': log_returns.loc['2023-01-01 00:00:00':]\n}\n\ndef kou_pdf(x, mu, sigma, lamb, p, eta1, eta2, delta_t=1):\n    no_jump_pdf = (1 - lamb * delta_t) * norm.pdf(x, mu * delta_t, sigma * np.sqrt(delta_t))\n    upward_jump_pdf = (lamb * delta_t * p * eta1 * np.exp((sigma**2 * eta1**2 * delta_t) / 2) *\n                       np.exp(-eta1 * (x - mu * delta_t)) *\n                       norm.cdf((x - mu * delta_t - sigma**2 * eta1 * delta_t) / (sigma * np.sqrt(delta_t))))\n    downward_jump_pdf = (lamb * delta_t * (1 - p) * eta2 * np.exp((sigma**2 * eta2**2 * delta_t) / 2) *\n                         np.exp(eta2 * (x - mu * delta_t)) *\n                         norm.cdf((-x + mu * delta_t + sigma**2 * eta2 * delta_t) / (sigma * np.sqrt(delta_t))))\n    return no_jump_pdf + upward_jump_pdf + downward_jump_pdf\n\ndef kou_cdf(x, mu, sigma, lamb, p, eta1, eta2, delta_t=1):\n    cdf_values = np.array([np.sum(kou_pdf(xi, mu, sigma, lamb, p, eta1, eta2, delta_t) * delta_t) for xi in x])\n    return cdf_values\n\ndef kou_log_likelihood(params, data):\n    mu, sigma, lamb, p, eta1, eta2, delta_t = params\n    pdf_values = kou_pdf(data, mu, sigma, lamb, p, eta1, eta2, delta_t)\n    return -np.sum(np.log(pdf_values + 1e-9))  # Add small constant to avoid log(0)\n\ndef fit_kou_model(log_returns):\n    mu, sigma = np.mean(log_returns), np.std(log_returns)\n    threshold = 3 * sigma\n    jump_indices = np.where(np.abs(log_returns - mu) > threshold)[0]\n    lamb = len(jump_indices) / len(log_returns)\n    jump_sizes = log_returns[jump_indices] - mu\n\n    positive_jumps = jump_sizes[jump_sizes > 0]\n    negative_jumps = -jump_sizes[jump_sizes < 0]\n    eta1 = 1 / np.mean(positive_jumps) if len(positive_jumps) > 0 else 1.0\n    eta2 = 1 / np.mean(negative_jumps) if len(negative_jumps) > 0 else 1.0\n    p = len(positive_jumps) / len(jump_sizes) if len(jump_sizes) > 0 else 0.5\n\n    initial_params = [mu, sigma, lamb, p, eta1, eta2, 1]\n    return minimize(kou_log_likelihood, initial_params, args=(log_returns,), method='Nelder-Mead').x\n\ndef fit_distributions(log_returns):\n    distributions = {\"normal\": stats.norm, \"t\": stats.t, \"gpd\": stats.genpareto}\n    return {name: (dist, dist.fit(log_returns)) for name, dist in distributions.items()}\n\ndef goodness_of_fit(data, distribution, params):\n    cdf = lambda x: distribution.cdf(x, *params)\n    return stats.kstest(data, cdf)\n\ndef evaluate_best_fit(log_returns, fitted_params, kou_params):\n    gof_results = {name: goodness_of_fit(log_returns, dist, params) for name, (dist, params) in fitted_params.items()}\n    gof_results[\"kou\"] = stats.kstest(log_returns, lambda x: kou_cdf(x, *kou_params))\n    return min(gof_results, key=lambda k: gof_results[k][0]), gof_results\n\ndef plot_best_fit(log_returns, best_fit, best_params, fitted_params, kou_params):\n    x = np.linspace(min(log_returns), max(log_returns), 1000)\n    if best_fit == \"kou\":\n        pdf = kou_pdf(x, *kou_params)\n    else:\n        pdf = fitted_params[best_fit][0].pdf(x, *best_params)\n    \n    plt.figure(figsize=(5, 3))\n    plt.hist(log_returns, bins=50, density=True, alpha=0.6, color='g')\n    plt.plot(x, pdf, 'k', linewidth=2)\n    plt.title(f\"Best fit distribution: {best_fit}\")\n    plt.show()\n\nfor condition, prices in market_conditions.items():\n    kou_params = fit_kou_model(log_returns)\n    fitted_params = fit_distributions(log_returns)\n    best_fit, gof_results = evaluate_best_fit(log_returns, fitted_params, kou_params)\n    best_params = kou_params if best_fit == \"kou\" else fitted_params[best_fit][1]\n    plot_best_fit(log_returns, best_fit, best_params, fitted_params, kou_params)\n    print(f\"Best fitting distribution for {condition}: {best_fit}\")\n    if best_fit == \"kou\":\n        print(f\"Kou model parameters for {condition}: {kou_params}\")\n    else:\n        print(f\"{best_fit} parameters for {condition}: {best_params}\")\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-04T16:14:07.850180Z","iopub.execute_input":"2024-08-04T16:14:07.851046Z","iopub.status.idle":"2024-08-04T16:16:36.612102Z","shell.execute_reply.started":"2024-08-04T16:14:07.851014Z","shell.execute_reply":"2024-08-04T16:16:36.611258Z"},"trusted":true},"execution_count":null,"outputs":[]}]}